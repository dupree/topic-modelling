{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/rshringi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import nltk\n",
    "from nltk import ne_chunk_sents, ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "# from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "# supressing the INFO log messages while training the model below to keep notebook from being unresponsive\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "This section is about unsupervised learning with text data.\n",
    "We'll be using a relatively small dataset, consisting of 3430 documents and a vocabulary of 6906 words drawn from the daily kos blog around 2004. \n",
    "\n",
    "You can download the data [here](https://s3-eu-west-1.amazonaws.com/lastmilecoding/exercise.tar.gz).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian mixture model\n",
    "\n",
    "We're going to model the documents as bags of words, with a bayesian mixture model.\n",
    "The documents are modeled using $K$ topics.\n",
    "The assignment of a document to a topic is modeled by the latent variable $z_d$.\n",
    "\n",
    "The topics are drawn from a categorical distribution with parameters $\\theta$, where $\\theta$ is drawn from a Dirichlet prior with parameter $\\alpha$.\n",
    "\n",
    "Each topic specifies a categorical distribution over words. The prior on each of these distributions is a Dirichlet with parameter $\\gamma$. \n",
    "\n",
    "\n",
    "The figure below shows this in a graphical model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Graphical Model](http://i.imgur.com/AAGnKZ7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's denote with $y_n$ the observations (i.e. the documents we see in the corpus).\n",
    "The conditional likelihood is:\n",
    "\n",
    "$$p(y_n|z_n \\negmedspace = \\negmedspace k,\\beta) = p(y_n|\\beta_k) = p(y_n|\\beta_{z_n})$$\n",
    "\n",
    "And we have a prior:\n",
    "\n",
    "$$p(\\beta_k)$$\n",
    "\n",
    "And a (latent) categorical assignment probability:\n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|\\theta) = \\theta_k$$\n",
    "\n",
    "which has a Dirichlet prior:\n",
    "\n",
    "$$p(\\theta|\\alpha) = Dir(\\alpha)$$\n",
    "\n",
    "Which gives our latent posterior:\n",
    "\n",
    "$$ p(z_n\\negmedspace=\\negmedspace k|y_n,\\theta,\\beta) \\quad \\propto p(z_n\\negmedspace =\\negmedspace k|\\theta)p(y_n|z_n \\negmedspace = \\negmedspace k,\\beta) \\quad \\propto \\theta_k p(y_n|\\beta_{z_n}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling\n",
    "We will explore this model by drawing from the posterior using MCMC, specifically Gibbs sampling.\n",
    "\n",
    "We will alternately sample the three types of variables, & iterate this procedure multiple times.\n",
    "\n",
    "First we'll sample the component parameters:\n",
    "\n",
    "$$p(\\beta_k|y,z) \\quad \\propto p(\\beta_k) \\prod_{n:z_n=k} p(y_n|\\beta_k) $$\n",
    "\n",
    "Then the latent allocations of documents to topics:\n",
    "$$ p(z_n \\negmedspace= \\negmedspace k|y_n,\\theta,\\beta) \\quad \\propto \\theta_k p(y_n|\\beta_{z_n})$$\n",
    "\n",
    "and then the mixing proportions:\n",
    "\n",
    "$$p(\\theta|z,\\alpha) = p(\\theta|\\alpha)p(z|\\theta) = \\mathrm{Dir}\\left(\\frac{c_k+\\alpha_k}{\\sum_j c_j + \\alpha_j}\\right)$$\n",
    "\n",
    "where $c_k$ are the counts for mixture component $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Collapsed Gibbs Sampler\n",
    "\n",
    "We marginalise over $\\theta$. (You do not need to derive this result).\n",
    "N.B. the notation $c_{-n}$ indicates all indices _except_ $n$. \n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|z_{-n},\\alpha) = \\frac{\\alpha + c_{-n,k}}{\\sum_j \\alpha + c_{-n,j}}$$\n",
    "\n",
    "which gives the _collapsed_ gibbs sampler for the latent assignments:\n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|y_n,z_{-n},\\beta,\\alpha) \\propto p(y_n|\\beta_k) \\frac{\\alpha + c_{-n,k}}{\\sum_j \\alpha + c_{-n,j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "The documents have been split into two corpora, `A` and `B`.\n",
    "\n",
    "The array `words` is a list of all the words in both corpora.\n",
    "The matrices `A` and `B` are the train and test corpora, respectively. \n",
    "Each has 3 columns, there is one row for each unique word in each document.\n",
    "\n",
    "The first column is the document index, second is the word index (corresponding to `words`), and the third is the number of times that word appears in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.load('./data/mat_A.npy')\n",
    "B = np.load('./data/mat_B.npy')\n",
    "words = np.load('./data/words.npy')\n",
    "\n",
    "W = np.max(np.hstack((A[:,1],B[:,1]))) + 1   # number of unique words\n",
    "D = np.max(A[:,0]) + 1   # number of documents in A\n",
    "K = 20 # number of mixture components we will use\n",
    "\n",
    "alpha = 10  # parameter of the Dirichlet over mixture components\n",
    "gamma = 0.1 # parameter of the Dirichlet over words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below defines the following matrices:\n",
    " * `sd` : the mixture component assignment of each document\n",
    " * `swk` : K multinomial distributions over W unique words\n",
    " * `sk_docs` : the number of documents assigned to each mixture component\n",
    " * `sk_words` : the number of words assigned to mix component `k` accross all docs\n",
    " \n",
    "These are initialised by assigning each document to a mixture component at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    sd = np.floor(K*np.random.random((D,1))).astype(int)   \n",
    "    swk = np.zeros((W,K))              \n",
    "    sk_docs = np.zeros((K,1)) \n",
    "\n",
    "    for d in np.arange(D): \n",
    "        w = A[A[:,0]==d,1]      # unique words in doc d\n",
    "        c = A[A[:,0]==d,2]      # counts\n",
    "        k = sd[d]               # doc d is in mixture k\n",
    "        swk[w,k] = swk[w,k] + c # num times word w is assigned to mixture component k\n",
    "        sk_docs[k] = sk_docs[k] + 1\n",
    "\n",
    "    sk_words = np.sum(swk,axis=0).T\n",
    "    return sd, swk, sk_docs, sk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code starts from this initial state & then performs a number of gibbs sampling sweeps. \n",
    "We will use the collapsed Gibbs sampler, which uses the trick of excluding the current document's counts before calculating the posterior and resampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibbs sweep : 0\n",
      "gibbs sweep : 1\n",
      "gibbs sweep : 2\n",
      "gibbs sweep : 3\n",
      "gibbs sweep : 4\n",
      "gibbs sweep : 5\n",
      "gibbs sweep : 6\n",
      "gibbs sweep : 7\n",
      "gibbs sweep : 8\n",
      "gibbs sweep : 9\n"
     ]
    }
   ],
   "source": [
    "sd, swk, sk_docs, sk_words = init()\n",
    "# This makes a number of Gibbs sampling sweeps through all docs and words\n",
    "num_sweeps = 10\n",
    "for i_sweep in np.arange(num_sweeps): \n",
    "    print(\"gibbs sweep : {0}\".format(i_sweep))\n",
    "    for d in np.arange(D):\n",
    "        w = A[A[:,0]==d,1]      # unique words in doc d\n",
    "        c = A[A[:,0]==d,2]      # counts\n",
    "\n",
    "        # remove doc d's contributions from count tables\n",
    "        swk[w,sd[d]] = swk[w,sd[d]] - c \n",
    "        sk_docs[sd[d]] = sk_docs[sd[d]] - 1 \n",
    "        sk_words[sd[d]] = sk_words[sd[d]] - np.sum(c) \n",
    "        \n",
    "        # log probability of doc d under each mixture component\n",
    "        lb = np.zeros(K)    \n",
    "        for k in np.arange(K):\n",
    "            ll = np.dot(c,( np.log(swk[w,k]+gamma) - np.log(sk_words[k] + gamma*W) ))\n",
    "            lb[k] = np.log(sk_docs[k] + alpha) + ll\n",
    "\n",
    "        # assign doc d to a new component\n",
    "        b = np.exp(lb-np.max(lb))  \n",
    "        kk = sample_discrete(b)  \n",
    "\n",
    "        # add back doc d's contributions from count tables\n",
    "        swk[w,kk] = swk[w,kk] + c \n",
    "        sk_docs[kk] = sk_docs[kk] + 1 \n",
    "        sk_words[kk] = sk_words[kk] + np.sum(c)\n",
    "        sd[d] = kk        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd, swk, sk_docs, sk_words = init()\n",
    "\n",
    "# print (sd, type(sd), sd.shape)\n",
    "# print (swk[10], type(swk), swk.shape)\n",
    "# print (sk_docs, type(sk_docs), sk_docs.shape, np.sum(sk_docs))\n",
    "# print (sk_words, type(sk_words), sk_words.shape, np.sum(sk_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the helper function: sample_discrete()\n",
    "- check and add comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below draws a sample from an unnormalised discrete distribution.\n",
    "def sample_discrete(b):\n",
    "# np.random.random() return random floats in the half-open interval [0.0, 1.0). Thus making r = [0.0, np.sum(b))\n",
    "    r = np.sum(b)*np.random.random();\n",
    "# copying the contents of first element of b into a\n",
    "    a = b[0].copy()\n",
    "    i = 0\n",
    "# till a is less than the random value in r, update a as sum of a and next item in b, starting from b[1]\n",
    "    while a < r:\n",
    "        i += 1\n",
    "        a += b[i]\n",
    "    return i\n",
    "\n",
    "# the loop exits when a becomes higher than r, it returns that index(i) of b where summation from 0 to i of b[] \n",
    "# becomes greater than the number r\n",
    "\n",
    "# deductions: \"i\" is integer value < = len(b)-1 or (b.size - 1) and range for i = [0, len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : \n",
    "1. **Explain what the code above does. Are you happy with this model? What are its deficiencies?**\n",
    " \n",
    "**Answer:**: \n",
    "- Above code attempts to implement collapsed Gibb's sampler for LDA (latent Dirichlet Allocation).\n",
    "- Given a corpus of documents as distribution of words it aims to achieve the task of Topic Modelling\n",
    "\n",
    "**K** = 20 # number of Topics\n",
    "\n",
    "**sd** : Each row corresponds to specific document and the value of topic that the specific document belongs to, ranges from 0-19 (no of topics => K = 20)\n",
    "<br>One document is assigned to only one topic. ***However, ideally a document should have a distribution over topics***\n",
    "<br>dimension : (No. of unique docs, 1 ) = (2000, 1)\n",
    " \n",
    "**swk** : Distribution of K-topics for each word, over the entire corpus. \n",
    "<br> eg. [ 2.  3.  2.  0.  3.  1.  0.  1.  0.  0.  2. 14.  0.  5.  0.  0.  1.  0.  1. 11.]\n",
    "<br> dimension : (No. of unique words, No. of topics ) = (6906, 20)\n",
    "\n",
    "**sk_docs** : 20 entries one for each K topic, contains total no. of docs assigned per topic. \n",
    "<br>dimension : (No. of topics, 1 ) = (20, 1)\n",
    "<br> np.sum(sk_docs) = Total number of documents, **implying that each document belongs to only one Topic.**\n",
    "\n",
    "**sk_words** : 20 entries, one for each K topic. Shows number of words assigned to each topic over entire corpus. One word can be assigned to multiple topics.\n",
    "<br> eg. [11984. 12039. 13412. 11942. 13266. 15652. 12111. 13249. 14146. 15024.12709. 14054. 16948. 13121. 11197. 13787. 13317. 15356. 15046. 13538.] \n",
    "<br>dimension : (No. of topics, ) = (20,)\n",
    "<br>np.sum(sk_words) = 271898.0 > W ; **implying that one words belongs to one or more topics.**\n",
    " \n",
    "<br> The ***init()*** function:\n",
    "- Initialises vairables sd, swk, sk_docs, sk_words at random. \n",
    "- It begins by generating random numbers in range (0,1]*K for total no of documents(D) in train set, and initializes swk, sk_docs as numpy arrays filled with zeros. \n",
    "- Then it iterates over every document in range (0..D) and performs word count and topic count based on initial random distribution in sd.\n",
    "\n",
    "- w = A[A[:,0]==d,1] ; iterates over A, looks for all the (unique)words with the document_id 'd' and stores their word_index in w\n",
    "- c = A[A[:,0]==d,2]; iterates over A, looks for all the (unique)words with the document_id 'd' and stores their count in c \n",
    "- k = sd[d]; get the random topic assigned to document d\n",
    "- swk[w,k] = swk[w,k] + c ; Update the value of specific word count with the specific topic\n",
    "- sk_docs[k] = sk_docs[k] + 1 ; Increase the document associated with the topic 'k' by one\n",
    "- sk_words = np.sum(swk,axis=0).T; Gets the column wise sum in swk matrix to get sum of words per topic. \n",
    "\n",
    "<br> Now we **train** this model by Gibb's sampling technique. The algorithm works as follows:\n",
    "- we predefine no. of times Gibbs sampling is performed\n",
    "- for each iteration of Gibb's sampling, iterate over the each document in corpus.\n",
    "- we first calculate the contribution of words and topics for this document \"d\" and remove it from the matrices swk, sk_words, sk_docs. \n",
    "- The **idea** is to remove the contribution of document \"d\" and then re-sample it's contribution assuming that the contribution from all other documents stay fixed in swk, sk_words, sk_docs.\n",
    "**Resample the contribution of a document \"d\" given these existing data/matrices which acts as a Prior.**\n",
    "<br>\n",
    "\n",
    "- log probability of document \"d\" for belonging to a topic \"K\" is then calculated via iteration over each topic. \n",
    "- Variable \"ll\" calculates the dot product of \"c\" count of all uniques words with a log(fraction). The fraction represents probability of a specific word belonging to a specific topic, this includes the dirichlet coefficients acting as smoothing factors in case of values going to zero.\n",
    "\n",
    "- variable \"b\" represents the probablity distribution of document \"d\" to belong to \"K\" topics. [0..19]\n",
    "- kk = sample_discrete(b) returns an integer from range [0, 20) indicating the new topic doc \"d\" belongs to\n",
    "\n",
    "- once we have new topic \"kk\", we update the contribution of doc \"d\" throughout the matrices swk, sk_words, sk_docs and repeat the iteration for next documents\n",
    "\n",
    "- Although we start with random values but over a period of time (say after N sweeps - we forget the intialization if we run the sampler long enough) the values stabilize to give good co-relation between the calculated values in swk, sk_words, sk_docs with the observation (training data).  \n",
    "\n",
    "***Deficiences:*** \n",
    "<br>\n",
    "- Each document (data in **sd**) is assigned only one Topic while it ideally should be a distribution over k-topics similar to as the words are associated with multiple topics in **swk**.\n",
    "\n",
    "- Above algorithm is sampling a new(single) topic once we have re-sampled topic distributions for a specific document:<br>\n",
    "**kk = sample_discrete(b)**\n",
    "\n",
    "This returns a single number between [0,20) but is not stable and gives different values on subsequent runs.\n",
    "\n",
    "- Running on test data to infer new topic for say document zero of set B, call to sample_discrete(b) gives different topic assignment on subsequent runs with the fixed input \"b\" - topic distribution of document\n",
    "\n",
    "\n",
    "**Suggestions:**\n",
    "<br> (pseudocode)\n",
    "- **Iterate** for each document in corpus:\n",
    "    - for each word in document:\n",
    "        - reset it's contribution(count) values from topic-word distribution matrix and document-topic matrix accordingly\n",
    "        - based on values in word-topic matrix and document-topic matrix, recalculate two probabilities to represent: **how much each Topic likes this word** and **how much this document likes a specific topic**\n",
    "        - Product of above two quantities gives us the probaility distribution of the specific word with topics, the most probable topic can become a **new topic assignment** of this word. \n",
    "        - update the conribution of the word in topic-word distribution matrix and document-topic matrix\n",
    "        - repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "**2. Calculate the log probability of the first document in corpus B. What does this number mean?**\n",
    "<br>**Ans:**\n",
    "<br> I have calculated below the log probability of the first document in corpus B **under each mixture model**. This number shows the log probability value **distribution(numbers do NOT add up to 1)** of topics for first document in corpus B based on the, word-topic distribution and doc-topic distribution learnt during training through Gibbs sampling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41532258 0.04233871 0.125      0.29032258 0.04637097 0.03629032\n",
      " 0.77016129 0.07862903 0.3266129  0.50806452 0.13104839 0.05645161\n",
      " 0.0483871  0.03830645 0.27217742 1.         0.08064516 0.05040323\n",
      " 0.02822581 0.09072581]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#  log probability of the first document in corpus B under each mixture model\n",
    "d = 0\n",
    "K = 20\n",
    "w = B[B[:,0]==d,1]      # unique words in doc d\n",
    "c = B[B[:,0]==d,2]      # counts\n",
    "\n",
    "# log probability of doc d under each mixture component\n",
    "lb = np.zeros(K)    \n",
    "for k in np.arange(K):\n",
    "    ll = np.dot(c,( np.log(swk[w,k]+gamma) - np.log(sk_words[k] + gamma*W) ))\n",
    "    lb[k] = np.log(sk_docs[k] + alpha) + ll\n",
    "\n",
    "# assign doc d to a new component\n",
    "b = np.exp(lb-np.max(lb))  \n",
    "kk = sample_discrete(b)\n",
    "\n",
    "print (b)\n",
    "print (kk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "**3. Calculate the per-word perplexity of the test corpus. What does this number mean? Is it good?**\n",
    "\n",
    "- What does this number mean?\n",
    "<br> Perplexity, is an intrinsic way of evaluation of any language model(over here our Topic model). It's based on the idea that given the specific model parameteres how likely is to observe the given document/corpus. Also known as hold one out test(where test set is kept separate and is not used in training the model).\n",
    "<br> The higher the log likelihood of unseen documents the better the model. Preplexity is computed as the decreasing function of log likelihood of unseen documents. Thus the lower the perplexity the better the model.\n",
    "<br> \n",
    "- On reading more about perplexity computation, I did get to know about methods such as LR:Left-to-right. CS: Chib-style but couldn't implement them in given time.\n",
    "<br> Sources : http://dirichlet.net/pdf/wallach09evaluation.pdf\n",
    "<br> http://qpleple.com/perplexity-to-evaluate-topic-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferences :\n",
    "- Retrieving **top word distributions for each Topic** for above model \n",
    "- Using external libraries to perform topic modelling on **this data-set (mat_A.npy)**\n",
    "- https://pythonhosted.org/lda/\n",
    "- https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: november poll house bush senate republicans kerry polls governor electoral \n",
      "Topic 1: senate republicans democrats investigation subpoena grand records federal law subpoenas \n",
      "Topic 2: nader ballot voters general republican bunning election state party voter \n",
      "Topic 3: bush kerry poll voters percent polls general president election polling \n",
      "Topic 4: bush kerry endorsement papers general oct news president editorial newspaper \n",
      "Topic 5: endorsement afscme unions dean labor seiu candidate union democratic clark \n",
      "Topic 6: kerry bush democratic campaign dean party media democrats edwards general \n",
      "Topic 7: bush kerry service guard records national military texas bushs kerrys \n",
      "Topic 8: bush president administration tax kerry republicans states jobs house people \n",
      "Topic 9: senate race house republican elections democratic state democrats percent district \n",
      "Topic 10: kerry bush poll results general polls pdf surveyusa percent voters \n",
      "Topic 11: delay committee house ethics texas democrats republican investigation elections bell \n",
      "Topic 12: research cell bush science stem reagan years mars dna life \n",
      "Topic 13: bush kerry ads campaign state percent general illinois firefighters bushs \n",
      "Topic 14: kerry dean edwards clark primary poll democratic gephardt lieberman iowa \n",
      "Topic 15: bush iraq war administration president people american general military house \n",
      "Topic 16: million campaign donors kerry bush candidates matsunaka dkos money funds \n",
      "Topic 17: campaign sunday national john sens adviser chairman kerry senator edition \n",
      "Topic 18: oceana official department register fishing vote media turner pentagon civilians \n",
      "Topic 19: ryan republican senate republicans keyes bush illinois convention obama gop \n"
     ]
    }
   ],
   "source": [
    "for k in range(0,20):\n",
    "    swk_T = swk.T\n",
    "    print (\"Topic %s:\" % k, end =\" \")\n",
    "    topic_wordIndices = np.argsort(swk_T[k])[::-1][:10]\n",
    "    for word_id in topic_wordIndices:\n",
    "        print (words[word_id], end =\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data in the form ingestible by downstream LDA model\n",
    "\n",
    "X = np.zeros((D, words.size), dtype=int) # initialise a document-term matrix\n",
    "\n",
    "# Transforming the train set A in X(document-term matrix) where each row represents a document with counts of words in column, where \n",
    "# column number corresponds to word_index as stored in vocabulary @words\n",
    "\n",
    "for d in np.arange(D):\n",
    "    w = w = A[A[:,0]==d,1]      # unique words in doc d\n",
    "    c = A[A[:,0]==d,2] \n",
    "    for word_id in np.arange(len(w)):\n",
    "        X[d,w[word_id]] = int(c[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: people america time american years political white\n",
      "Topic 1: poll bush kerry percent polls voters polling\n",
      "Topic 2: november senate account governor electoral polls vote\n",
      "Topic 3: states state voters nader vote election general\n",
      "Topic 4: administration house bush officials white commission information\n",
      "Topic 5: november turnout exit west poll trouble parecommend\n",
      "Topic 6: iraq war bush administration united states american\n",
      "Topic 7: november voting general election planned soldier republicans\n",
      "Topic 8: media news ill blog press bloggers read\n",
      "Topic 9: senate campaign john bunning senator clinton news\n",
      "Topic 10: party democrats republican republicans democratic delay committee\n",
      "Topic 11: policy blades meteor reagan human research rights\n",
      "Topic 12: campaign money candidates million house race raised\n",
      "Topic 13: time people hard political election put matter\n",
      "Topic 14: dean edwards kerry primary democratic clark iowa\n",
      "Topic 15: law court bush national marriage state gay\n",
      "Topic 16: iraq war iraqi military soldiers troops forces\n",
      "Topic 17: bush tax jobs administration billion economy year\n",
      "Topic 18: bush kerry president general bushs john kerrys\n",
      "Topic 19: senate race house republican elections state seat\n"
     ]
    }
   ],
   "source": [
    "# https://pythonhosted.org/lda/\n",
    "# lda implements latent Dirichlet allocation (LDA) using collapsed Gibbs sampling.\n",
    "\n",
    "import lda\n",
    "vocab = words\n",
    "\n",
    "model = lda.LDA(n_topics=20, n_iter=2000, random_state=1)\n",
    "model.fit(X)  # model.fit_transform(X) is also available\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loglikelihood value: -2173745.586473\n",
      "perplexity value: 314.761886\n",
      "It can be observed that the loglikelihood value stabilizes over a period of time.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"loglikelihood value: %f\" % (model.loglikelihood()))\n",
    "\n",
    "log_perplexity =  (-1 * (model.loglikelihood()/len(vocab)))\n",
    "print (\"perplexity value: %f\" % (log_perplexity))\n",
    "\n",
    "plt.xlabel(\"iterations*10\")\n",
    "plt.ylabel(\"loglikelihood\")\n",
    "plt.plot(model.loglikelihoods_[5:])\n",
    "\n",
    "print (\"It can be observed that the loglikelihood value stabilizes over a period of time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming train set A into format to be used by Gensim\n",
    "\n",
    "#  each row of LIST X_corpus(document-term matrix) contains (word_id,count) where word_id is the identifier for a word in vocabulary\n",
    "#  count is the number of times that word appears in document\n",
    "\n",
    "X_corpus = [] \n",
    "\n",
    "# Transforming the train set A in X where each row represents a document with counts of words in column, where \n",
    "# column number corresponds to word_index as stored in vocabulary @words\n",
    "\n",
    "for d in np.arange(D):\n",
    "    w = w = A[A[:,0]==d,1]      # unique words in doc d\n",
    "    c = A[A[:,0]==d,2]\n",
    "    X_corpus.append(zip(w,c))\n",
    "    \n",
    "id2word = dict(enumerate(words.flatten(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=X_corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  u'0.035*\"bus\" + 0.019*\"millers\" + 0.018*\"jobless\" + 0.015*\"presidency\" + 0.015*\"tauzin\" + 0.015*\"admin\" + 0.014*\"bushrove\" + 0.013*\"economists\" + 0.012*\"bill\" + 0.012*\"echoed\"'),\n",
      " (1,\n",
      "  u'0.012*\"polish\" + 0.007*\"tim\" + 0.007*\"year\" + 0.007*\"metaphor\" + 0.007*\"blackwell\" + 0.006*\"proceed\" + 0.006*\"cent\" + 0.006*\"pentagons\" + 0.006*\"calistan\" + 0.006*\"confederate\"'),\n",
      " (2,\n",
      "  u'0.086*\"dealt\" + 0.076*\"edward\" + 0.063*\"primarily\" + 0.050*\"kerrey\" + 0.050*\"democrat\" + 0.041*\"clarify\" + 0.031*\"involving\" + 0.026*\"gep\" + 0.025*\"liebeck\" + 0.016*\"candidate\"'),\n",
      " (3,\n",
      "  u'0.024*\"camp\" + 0.012*\"tim\" + 0.010*\"pentagons\" + 0.009*\"shown\" + 0.008*\"harbor\" + 0.007*\"climate\" + 0.007*\"toll\" + 0.007*\"mess\" + 0.007*\"jindal\" + 0.007*\"weekly\"'),\n",
      " (4,\n",
      "  u'0.069*\"novak\" + 0.014*\"hours\" + 0.014*\"governments\" + 0.014*\"accomplishment\" + 0.014*\"elections\" + 0.013*\"politics\" + 0.013*\"sen\" + 0.013*\"republican\" + 0.012*\"polling\" + 0.010*\"volunteers\"'),\n",
      " (5,\n",
      "  u'0.047*\"sen\" + 0.032*\"spectacular\" + 0.028*\"oath\" + 0.021*\"illegally\" + 0.018*\"knowledge\" + 0.017*\"rva\" + 0.017*\"coattails\" + 0.016*\"bizarre\" + 0.016*\"primarily\" + 0.016*\"alex\"'),\n",
      " (6,\n",
      "  u'0.029*\"waited\" + 0.027*\"vibes\" + 0.024*\"absurd\" + 0.022*\"gharib\" + 0.018*\"toptier\" + 0.018*\"brazil\" + 0.018*\"balanced\" + 0.018*\"huh\" + 0.016*\"priority\" + 0.015*\"choices\"'),\n",
      " (7,\n",
      "  u'0.017*\"pentagons\" + 0.010*\"israeli\" + 0.009*\"amendments\" + 0.007*\"year\" + 0.007*\"tim\" + 0.006*\"facing\" + 0.006*\"supply\" + 0.005*\"lobbyists\" + 0.005*\"womans\" + 0.005*\"hoover\"'),\n",
      " (8,\n",
      "  u'0.049*\"politics\" + 0.043*\"perceived\" + 0.031*\"voter\" + 0.024*\"electing\" + 0.020*\"statements\" + 0.019*\"startspan\" + 0.018*\"volunteers\" + 0.017*\"polling\" + 0.017*\"number\" + 0.015*\"polled\"'),\n",
      " (9,\n",
      "  u'0.048*\"naacp\" + 0.027*\"uncovered\" + 0.024*\"krugman\" + 0.024*\"ball\" + 0.024*\"sharply\" + 0.020*\"dna\" + 0.017*\"jennings\" + 0.014*\"aristide\" + 0.013*\"wired\" + 0.012*\"president\"'),\n",
      " (10,\n",
      "  u'0.044*\"medals\" + 0.020*\"blocks\" + 0.018*\"international\" + 0.014*\"study\" + 0.014*\"weather\" + 0.014*\"shaping\" + 0.014*\"significance\" + 0.012*\"blogpac\" + 0.012*\"communities\" + 0.012*\"dozens\"'),\n",
      " (11,\n",
      "  u'0.031*\"admin\" + 0.020*\"hours\" + 0.020*\"whining\" + 0.014*\"replied\" + 0.013*\"officially\" + 0.013*\"commercials\" + 0.012*\"intellectual\" + 0.011*\"inform\" + 0.010*\"offers\" + 0.010*\"offices\"'),\n",
      " (12,\n",
      "  u'0.104*\"irans\" + 0.078*\"wapo\" + 0.025*\"america\" + 0.022*\"iraq\" + 0.021*\"militants\" + 0.016*\"troop\" + 0.015*\"sad\" + 0.014*\"soldier\" + 0.013*\"countries\" + 0.012*\"forced\"'),\n",
      " (13,\n",
      "  u'0.040*\"hours\" + 0.035*\"sen\" + 0.028*\"election\" + 0.023*\"monetary\" + 0.022*\"quoting\" + 0.020*\"candidate\" + 0.019*\"camp\" + 0.017*\"goopers\" + 0.016*\"republic\" + 0.015*\"democraticleaning\"'),\n",
      " (14,\n",
      "  u'0.018*\"attacking\" + 0.018*\"bills\" + 0.017*\"policies\" + 0.015*\"lacking\" + 0.012*\"forecast\" + 0.012*\"internals\" + 0.012*\"occasions\" + 0.011*\"seperate\" + 0.009*\"governing\" + 0.009*\"celebrity\"'),\n",
      " (15,\n",
      "  u'0.034*\"courage\" + 0.032*\"startspan\" + 0.018*\"loses\" + 0.018*\"flops\" + 0.017*\"voters\" + 0.014*\"laura\" + 0.014*\"suppression\" + 0.012*\"deciding\" + 0.012*\"duderino\" + 0.011*\"privacy\"'),\n",
      " (16,\n",
      "  u'0.031*\"gene\" + 0.028*\"falluja\" + 0.021*\"armstrong\" + 0.019*\"marilyn\" + 0.018*\"command\" + 0.017*\"gehlen\" + 0.016*\"kingelection\" + 0.016*\"sincerely\" + 0.016*\"stella\" + 0.016*\"citizenship\"'),\n",
      " (17,\n",
      "  u'0.126*\"bus\" + 0.077*\"kerrey\" + 0.030*\"gene\" + 0.029*\"presidency\" + 0.020*\"joel\" + 0.019*\"newly\" + 0.015*\"bushrove\" + 0.013*\"kerryedwards\" + 0.013*\"genuinely\" + 0.012*\"deaths\"'),\n",
      " (18,\n",
      "  u'0.050*\"parts\" + 0.038*\"democraticleaning\" + 0.035*\"republic\" + 0.034*\"republican\" + 0.031*\"democrat\" + 0.017*\"conservatism\" + 0.014*\"convenient\" + 0.012*\"goopers\" + 0.011*\"volunteers\" + 0.011*\"polish\"'),\n",
      " (19,\n",
      "  u'0.024*\"chemical\" + 0.023*\"securing\" + 0.016*\"guaranteed\" + 0.015*\"serves\" + 0.015*\"defending\" + 0.014*\"secretaries\" + 0.013*\"attached\" + 0.011*\"nation\" + 0.011*\"admin\" + 0.009*\"bunch\"')]\n",
      "('Perplexity: ', -7.74959255269985)\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 20 topics\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[X_corpus]\n",
    "\n",
    "# Compute Perplexity\n",
    "print('Perplexity: ', lda_model.log_perplexity(X_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "\n",
    "In this part, you will do **topic modeling on New York Times corpus**. You can download the corpus from here\n",
    "(https://www.kaggle.com/nzalake52/new-york-times-articles) . This part is free style, please show us what you got\n",
    "library is welcomed, in particlar we are happy to see\n",
    "- Sequential models\n",
    "- Respecting the document hierarchy, organization into sentences and paragraphs\n",
    "- Different sorts of attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data input and preprocessing :\n",
    "\n",
    "- before beginning with topic modelling, parsing the input and applying following **pre-processing**:\n",
    "    - remove the **URL** from each article as it will not add any value after tokenisation\n",
    "    - tokenisation, stop word removal\n",
    "    - proper noun(s) removal, as they will not provide additional value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the news articles file\n",
    "\n",
    "nyTimesFile = open('new-york-times-articles/nytimes_news_articles.txt')\n",
    "nyTimesFile.seek(0)\n",
    "nyTimesV1 = nyTimesFile.readlines()\n",
    "nyTimesArticles = []\n",
    "nyTimesURL = []\n",
    "\n",
    "# collecting and removing the URLs into a separate list \n",
    "for i in range(0, len(nyTimesV1)-1):\n",
    "    if re.findall('URL', nyTimesV1[i]) == []:\n",
    "        sent = sent + nyTimesV1[i]\n",
    "        if (re.findall('URL', nyTimesV1[i+1]) != []) and (i+1 < len(nyTimesV1)):\n",
    "            nyTimesArticles.append(sent.strip())\n",
    "    else:\n",
    "        sent = ''\n",
    "        nyTimesURL.append(nyTimesV1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### BEFORE preprocess article contents : \n",
      "WASHINGTON  Stellar pitching kept the Mets afloat in the first half of last season despite their of\n",
      "##### done with tokenisation and stop word removal. After article contents :\n",
      "['WASHINGTON', 'Stellar', 'pitching', 'kept', 'Mets', 'afloat', 'first', 'half', 'last', 'season']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words_caps = [word.title() for word in stop_words]\n",
    "# generating list of stop_words with title case to be removed from text as well\n",
    "stop_words.extend(stop_words_caps)\n",
    "\n",
    "# Here I define a tokenizer and stopword removal function\n",
    "# It returns individual tokens for each article after removing stop words\n",
    "\n",
    "def tokenize_and_stopWordsRemoval(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation) and words whose \n",
    "    # length is less than 2 characters - removes some of the uncatched stopwords  \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and len(token) > 2 and token not in stop_words :\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns\n",
    "\n",
    "print (\"#### BEFORE preprocess article contents : \")\n",
    "print (nyTimesArticles[0][:100])\n",
    "\n",
    "#tokenize\n",
    "%time tokenized_text = [tokenize_and_stopWordsRemoval(text) for text in nyTimesArticles]\n",
    "\n",
    "print (\"##### done with tokenisation and stop word removal. After article contents :\")\n",
    "print (tokenized_text[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 49s, sys: 91.7 ms, total: 2min 49s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%time processed_text = [strip_proppers_POS(text) for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens removed after removing Proper Nouns for Article 0:\n",
      "['WASHINGTON', 'Stellar', 'Manager', 'Terry', 'Wednesday', 'Wednesday', 'Washington', 'Nationals', 'Steven', 'Matz', 'Logan', 'Verrett', 'Max', 'Scherzer', 'Nationals', 'James', 'Loney', 'Nationals', 'Shawn', 'Kelley', 'Nationals', 'National', 'League', 'East', 'Curtis', 'Granderson', 'Scherzer', 'Granderson', 'Granderson', 'Scherzer', 'Alejandro', 'Aza', 'Scherzer', 'Asdrubal', 'Cabrera', 'Loney', 'Brandon', 'Nimmo', 'Nimmo', 'Granderson', 'Travis', 'Arnaud', 'Aza', 'General', 'Manager', 'Sandy', 'Alderson', 'Earlier', 'May', 'June', 'Verrett', 'Daniel', 'Murphy', 'Addison', 'Reed', 'Jeurys', 'Familia', 'Sean', 'Gilmartin', 'Gilmartin', 'Murphy', 'Verrett', 'Verrett', 'Verrett', 'Wednesday', 'Matz', 'Noah', 'Syndergaard', 'Syndergaard', 'Matz', 'Thursday', 'Chicago', 'Cubs']\n",
      "\n",
      "Tokens removed after removing Proper Nouns for Article 1:\n",
      "['Mayor', 'Bill', 'Blasio', 'Maya', 'Wiley', 'City', 'Hall', 'Complaint', 'Review', 'Board', 'New', 'York', 'City', 'Police', 'Department', 'Blasio', 'Richard', 'Emery', 'April', 'Ms.', 'Wiley', 'Blasio', 'Ms.', 'Wiley', 'Mr.', 'Blasio', 'Mr.', 'Blasio', 'Henry', 'Berger', 'Ms.', 'Wiley', 'Mr.', 'Blasio', 'Ms.', 'Wiley', 'Wednesday', 'Mr.', 'Blasio', 'Ms.', 'Wiley', 'Police', 'Department', 'Mr.', 'Blasio', 'Democrat', 'Mr.', 'Wiley', 'New', 'School', 'Manhattan', 'Wall', 'Street', 'Journal', 'Ms.', 'Wiley', 'City', 'Hall', 'Scott', 'Kleinberg', 'Saturday', 'Twitter', 'Facebook', 'DNA', 'Info', 'Facebook', 'Mr.', 'Kleinberg', 'Mr.', 'Kleinberg', 'Karen', 'Hinton']\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check : check the difference in list contents after removing proper nouns\n",
    "# diff() method prints the tokens removed after strip_proppers_POS() function call\n",
    "\n",
    "def diff(first, second):\n",
    "        second = set(second)\n",
    "        return [item for item in first if item not in second]\n",
    "\n",
    "print (\"Tokens removed after removing Proper Nouns for Article 0:\")\n",
    "print (diff(tokenized_text[0],processed_text[0]))\n",
    "\n",
    "print()\n",
    "print (\"Tokens removed after removing Proper Nouns for Article 1:\")\n",
    "print (diff(tokenized_text[1],processed_text[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 386 ms, sys: 56 ms, total: 442 ms\n",
      "Wall time: 441 ms\n"
     ]
    }
   ],
   "source": [
    "# Remove capitalization\n",
    "%time processed_text_lower = [[word.lower() for word in x] for x in processed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topic Model :\n",
    "- using **Gensim's** implementation of online LDA\n",
    "- Online Latent Dirichlet Allocation (OLDA) models as presented by Hoffman, Blei, Bach: Online Learning for Latent Dirichlet Allocation, NIPS 2010.\n",
    "- https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "#create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(processed_text_lower)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_text_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"said\" + 0.011*\"company\" + 0.010*\"percent\" + 0.007*\"would\" + 0.007*\"million\" + 0.007*\"year\" + 0.006*\"companies\" + 0.005*\"last\" + 0.005*\"also\" + 0.005*\"billion\"'),\n",
       " (1,\n",
       "  '0.012*\"said\" + 0.008*\"like\" + 0.007*\"one\" + 0.006*\"show\" + 0.005*\"new\" + 0.004*\"also\" + 0.004*\"first\" + 0.004*\"people\" + 0.004*\"time\" + 0.004*\"would\"'),\n",
       " (2,\n",
       "  '0.028*\"said\" + 0.007*\"police\" + 0.007*\"case\" + 0.007*\"law\" + 0.007*\"court\" + 0.006*\"students\" + 0.005*\"school\" + 0.005*\"one\" + 0.005*\"also\" + 0.004*\"would\"'),\n",
       " (3,\n",
       "  '0.016*\"said\" + 0.012*\"would\" + 0.008*\"campaign\" + 0.005*\"party\" + 0.005*\"could\" + 0.005*\"people\" + 0.005*\"political\" + 0.005*\"state\" + 0.004*\"president\" + 0.004*\"one\"'),\n",
       " (4,\n",
       "  '0.016*\"said\" + 0.008*\"like\" + 0.008*\"one\" + 0.005*\"people\" + 0.004*\"would\" + 0.004*\"time\" + 0.003*\"years\" + 0.003*\"get\" + 0.003*\"back\" + 0.003*\"could\"'),\n",
       " (5,\n",
       "  '0.028*\"said\" + 0.009*\"government\" + 0.008*\"people\" + 0.006*\"officials\" + 0.005*\"country\" + 0.005*\"military\" + 0.005*\"one\" + 0.004*\"security\" + 0.004*\"two\" + 0.004*\"also\"'),\n",
       " (6,\n",
       "  '0.015*\"said\" + 0.009*\"health\" + 0.005*\"cancer\" + 0.005*\"study\" + 0.005*\"drug\" + 0.005*\"people\" + 0.005*\"one\" + 0.005*\"patients\" + 0.005*\"care\" + 0.005*\"medical\"'),\n",
       " (7,\n",
       "  '0.009*\"fashion\" + 0.006*\"designer\" + 0.006*\"said\" + 0.005*\"like\" + 0.005*\"wear\" + 0.005*\"collection\" + 0.004*\"show\" + 0.004*\"brand\" + 0.004*\"one\" + 0.004*\"men\"'),\n",
       " (8,\n",
       "  '0.017*\"said\" + 0.010*\"first\" + 0.010*\"game\" + 0.009*\"team\" + 0.008*\"two\" + 0.007*\"season\" + 0.007*\"one\" + 0.007*\"last\" + 0.006*\"players\" + 0.006*\"games\"'),\n",
       " (9,\n",
       "  '0.014*\"said\" + 0.007*\"building\" + 0.007*\"city\" + 0.007*\"new\" + 0.006*\"million\" + 0.006*\"art\" + 0.005*\"space\" + 0.005*\"one\" + 0.005*\"also\" + 0.005*\"house\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lda_v1 = models.LdaModel(corpus, num_topics=10,id2word=dictionary,update_every=5,chunksize=10000,passes=50)\n",
    "lda_v1.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training OLDA with **multicore** support\n",
    "- Change the **number of topics = 20**, keeping other parameters same as lda_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51min 6s, sys: 1min 50s, total: 52min 56s\n",
      "Wall time: 22min 21s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "\n",
    "num_topics = 20\n",
    "\n",
    "dictionary_mlda = corpora.Dictionary(processed_text_lower)\n",
    "\n",
    "# filter out words that appear in 30 documents or less,\n",
    "# or in more than 70% of documents\n",
    "dictionary_mlda.filter_extremes(no_below=30, no_above=0.7)\n",
    "\n",
    "corpus_mlda = [dictionary_mlda.doc2bow(doc) for doc in processed_text_lower]\n",
    "\n",
    "%time lda_v2 = LdaMulticore(corpus_mlda, id2word=dictionary_mlda, chunksize=10000,passes=50,workers=4, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\"\"\"\n",
    "Function to get the top word distribution across the topics\n",
    "Calculate model Perplexity and Coherence score\n",
    "\n",
    "Parameters\n",
    "        ----------\n",
    "        lda_model : gensim models.LdaModel\n",
    "            The trained LdaModel\n",
    "        corpus : list\n",
    "            bag of words corpus for reference\n",
    "        dictionary : key-value pairs\n",
    "            Dictionary  a mapping between words and their integer ids\n",
    " \"\"\"\n",
    "\n",
    "def get_topics(lda_model, num_topics):\n",
    "    topics_matrix = lda_model.show_topics(formatted=False, num_topics=num_topics, num_words=20)\n",
    "    topics_matrix = np.array(topics_matrix)\n",
    "\n",
    "    topic_words = topics_matrix[:,1]\n",
    "    count = 0\n",
    "    for i in topic_words:\n",
    "        print('Topic {%d}:' % count),\n",
    "        print(\" \".join([str(word[0]) for word in i]))\n",
    "        count += 1\n",
    "        print()    \n",
    "    \n",
    "def evaluate_topic_model(lda_model, corpus, dictionary, text):\n",
    "    # Compute Perplexity\n",
    "    print('Perplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is, lower the better.\n",
    "    \n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=dictionary, coherence='c_v')\n",
    "    print (\"Pipeline parameters for C_V coherence\")\n",
    "    print (coherence_model_lda)\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic {0}:\n",
      "said company percent would million year companies last also billion new business years could financial money executive one market chief\n",
      "\n",
      "Topic {1}:\n",
      "said like one show new also first people time would work years music last world film way two even year\n",
      "\n",
      "Topic {2}:\n",
      "said police case law court students school one also would two federal public years state lawyer last people officers investigation\n",
      "\n",
      "Topic {3}:\n",
      "said would campaign party could people political state president one vote also voters even like many support election percent think\n",
      "\n",
      "Topic {4}:\n",
      "said like one people would time years get back could two day family make even first home around way many\n",
      "\n",
      "Topic {5}:\n",
      "said government people officials country military one security two also group american last killed would many war attacks police city\n",
      "\n",
      "Topic {6}:\n",
      "said health cancer study drug people one patients care medical many drugs research doctors found years would could also scientists\n",
      "\n",
      "Topic {7}:\n",
      "fashion designer said like wear collection show brand one men says exhibition also look dress bonds art jewelry pieces clothes\n",
      "\n",
      "Topic {8}:\n",
      "said first game team two season one last players games second play three time would year win back points home\n",
      "\n",
      "Topic {9}:\n",
      "said building city new million art space one also house two room home like years museum including open project homes\n",
      "\n",
      "Perplexity:  -8.581634745993043\n",
      "Pipeline parameters for C_V coherence\n",
      "Coherence_Measure(seg=<function s_one_set at 0x7fc88adc99d8>, prob=<function p_boolean_sliding_window at 0x7fc88adc9bf8>, conf=<function cosine_similarity at 0x7fc857c092f0>, aggr=<function arithmetic_mean at 0x7fc857c09840>)\n",
      "\n",
      "Coherence Score:  0.40627374328688093\n"
     ]
    }
   ],
   "source": [
    "# check the topic-word distribution and perplexity scores for the above two LDA models\n",
    "\n",
    "get_topics(lda_v1, 10)\n",
    "\n",
    "evaluate_topic_model(lda_v1, corpus, dictionary, processed_text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic {0}:\n",
      "would law also could financial court board money decision government last company legal new bank case million whether deal executive\n",
      "\n",
      "Topic {1}:\n",
      "first father graduated year also couple two son last received mother race married bride daughter second groom met time tennis\n",
      "\n",
      "Topic {2}:\n",
      "like show would time first also new people film think years way story work know even book life two music\n",
      "\n",
      "Topic {3}:\n",
      "government country military officials american would also security war political people last forces group two countries president minister foreign attacks\n",
      "\n",
      "Topic {4}:\n",
      "art work artists museum million artist works exhibition show collection director gallery new auction contemporary pieces also year design sale\n",
      "\n",
      "Topic {5}:\n",
      "school students schools city children water people year many education student public residents state high parents fire black college local\n",
      "\n",
      "Topic {6}:\n",
      "people family gay police man killed gun shooting mother told would two son transgender night attack died home group back\n",
      "\n",
      "Topic {7}:\n",
      "city building two house room home new space also apartment area feet park property hotel years three open buildings around\n",
      "\n",
      "Topic {8}:\n",
      "team game players first season games two play points last player teams series time would league goal three second coach\n",
      "\n",
      "Topic {9}:\n",
      "state would bill law states delegates vote legislation could rules convention lawmakers laws measure federal rights support governor people majority\n",
      "\n",
      "Topic {10}:\n",
      "percent million year company billion market companies last oil prices would investors years economy business new growth stock quarter first\n",
      "\n",
      "Topic {11}:\n",
      "police case court two officers investigation lawyer years charges trial also prosecutors judge office prison former would criminal last according\n",
      "\n",
      "Topic {12}:\n",
      "food article restaurant recipe nytimes.com chicken cooking chef misstated may make also restaurants fish eat meat cook recipes last name\n",
      "\n",
      "Topic {13}:\n",
      "first season two game hit last runs yankees home run three second third inning innings games team six five right\n",
      "\n",
      "Topic {14}:\n",
      "like time would years first made way back even around day two also could people much see still look fashion\n",
      "\n",
      "Topic {15}:\n",
      "people like would many even years work percent get much think may want could make time say going women need\n",
      "\n",
      "Topic {16}:\n",
      "health drug cancer medical patients would drugs also could doctors hospital found vehicles cars officials two safety care test tests\n",
      "\n",
      "Topic {17}:\n",
      "company million new year companies also business executive last chief sales customers like industry service would products week including online\n",
      "\n",
      "Topic {18}:\n",
      "campaign would party voters political election presidential president vote also candidate democratic could support percent primary former people state race\n",
      "\n",
      "Topic {19}:\n",
      "people like media news online also social new information use technology could used app data world many digital would users\n",
      "\n",
      "Perplexity:  -7.935223746516602\n",
      "Pipeline parameters for C_V coherence\n",
      "Coherence_Measure(seg=<function s_one_set at 0x7fc88adc99d8>, prob=<function p_boolean_sliding_window at 0x7fc88adc9bf8>, conf=<function cosine_similarity at 0x7fc857c092f0>, aggr=<function arithmetic_mean at 0x7fc857c09840>)\n",
      "\n",
      "Coherence Score:  0.5018889033400405\n"
     ]
    }
   ],
   "source": [
    "get_topics(lda_v2, 20)\n",
    "evaluate_topic_model(lda_v2, corpus_mlda, dictionary_mlda, processed_text_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference : lda_v1 v/s lda_v2\n",
    "- **Coherence Score**(C_V coherence) improves from 0.40 to 0.50, on increasing **num_topics to 20** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet allocation using Collapsed Gibb's Sampling:\n",
    "- https://pythonhosted.org/lda/api.html\n",
    "- Another external library with an implementation of Collapsed Gibb's Sampling for LDA\n",
    "- Different set of perplexity score can be seen\n",
    "- It can also be seen that the model converges after around 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the existing preprocessed text into list of article strings\n",
    "# To be provided as input to CountVectorizer\n",
    "\n",
    "concat_article = [' '.join(article) for article in processed_text_lower]\n",
    "concat_corpus = ' '.join(concat_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(max_df=0.8, max_features=55000, min_df=50)\n",
    "\n",
    "doc_term_matrix = vec.fit_transform(concat_article)\n",
    "vocab = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshringi/anaconda3/envs/py3/lib/python3.7/site-packages/lda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x7fc84e830b70>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lda\n",
    "\n",
    "\"\"\"Latent Dirichlet allocation using collapsed Gibbs sampling\n",
    "\n",
    "Parameters:\n",
    "    --------\n",
    "    n_topics : int\n",
    "\n",
    "        Number of topics\n",
    "\n",
    "    n_iter : int, default 2000\n",
    "\n",
    "        Number of sampling iterations\n",
    "\n",
    "    alpha : float, default 0.1\n",
    "\n",
    "        Dirichlet parameter for distribution over topics\n",
    "\n",
    "    eta : float, default 0.01\n",
    "\n",
    "        Dirichlet parameter for distribution over words\n",
    "\n",
    "    random_state : int or RandomState, optional\n",
    "\n",
    "        The generator used for the initial topics.\n",
    "\"\"\"\n",
    "\n",
    "model = lda.LDA(n_topics=20, n_iter=1000, random_state=1)\n",
    "\n",
    "model.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: said game first two season games second three team last run series hit one\n",
      "Topic 1: food like restaurant also wine make day com recipe one water cooking chef chicken\n",
      "Topic 2: said city two building one room house home three area new space also years\n",
      "Topic 3: campaign said party voters election would presidential vote state political democratic candidate primary percent\n",
      "Topic 4: would said law court state federal could case legal decision government bill rules rights\n",
      "Topic 5: said company companies new like technology people also online business industry service customers use\n",
      "Topic 6: said people military officials killed attacks group attack security fire government two city gun\n",
      "Topic 7: art said work fashion museum like artist artists collection one works show first designer\n",
      "Topic 8: said government country would political president american also countries european world new foreign minister\n",
      "Topic 9: said team players year last first two sports game one player top play round\n",
      "Topic 10: like one would even think much way could people time make get going good\n",
      "Topic 11: women gay father graduated sex married couple mother transgender received also son daughter men\n",
      "Topic 12: percent million year company billion said market financial last companies investors prices money would\n",
      "Topic 13: said would also company former million board chief statement one executive two last according\n",
      "Topic 14: said one would like time people family years life back day first never two\n",
      "Topic 15: article news media book wrote editor public nytimes social published name one books com\n",
      "Topic 16: show music film new also first movie like year series shows play season one\n",
      "Topic 17: school students said year schools percent people children workers many college work years education\n",
      "Topic 18: said health drug medical cancer study patients found years care research also one drugs\n",
      "Topic 19: said police case officers charges year court trial prison prosecutors two man criminal years\n",
      "Model loglikelihood value: -23461933.329302 \n",
      "It can be observed that the loglikelihood value stabilizes over a period of time.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_topic_description(model, vocab):\n",
    "    topic_word = model.topic_word_  # model.components_ also works\n",
    "    n_top_words = 15\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "        print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "\n",
    "    print (\"Model loglikelihood value: %f \" % model.loglikelihood())\n",
    "    \n",
    "    plt.xlabel(\"iterations*10\")\n",
    "    plt.ylabel(\"loglikelihood\")\n",
    "    plt.plot(model.loglikelihoods_[5:])\n",
    "    \n",
    "get_topic_description(model, vocab)\n",
    "\n",
    "print (\"It can be observed that the loglikelihood value stabilizes over a period of time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for best parameter search:\n",
    "- Using sklearn **GridSearchCV** to search for over parameter space on  **LatentDirichletAllocation** model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshringi/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_topics': [10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source : https://medium.com/@yanlinc/how-to-build-a-lda-topic-model-using-from-text\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "# Do the Grid Search\n",
    "model.fit(doc_term_matrix)\n",
    "GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
    "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
    "             n_topics=None, perp_tol=0.1, random_state=None,\n",
    "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
    "       fit_params=None, iid=True, n_jobs=1,\n",
    "       param_grid={'n_topics': [10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring=None, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.5, 'n_components': 20}\n",
      "Best Log Likelihood Score:  -7285582.394132242\n",
      "Model Perplexity:  2161.615129623764\n"
     ]
    }
   ],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "- It's evident sklearn's LatentDirichletAllocation model is NOT achieving comparable results to gensim's OLDA\n",
    "    - max_iterations = 10 while GridSearch params could be the main reason\n",
    "    - due to limited time and computation resources, can not set high value for this parameter \n",
    "- Hoever both models agree on number of topics = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Dirichlet Process\n",
    "- Evaluate ***Hierarchial Dirichlet Process*** model available in Gensim\n",
    "- The core estimation code is directly adapted from the blei-lab/online-hdp from Wang, Paisley, Blei: Online Variational Inference for the Hierarchical Dirichlet Process, JMLR (2011).\n",
    "https://radimrehurek.com/gensim/models/hdpmodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "# split the corpus in test and train set : 20%-80%\n",
    "# ideally should be done random shuffle\n",
    "train_len = int(0.8 * len(corpus_mlda))\n",
    "hdp_corpus_train = corpus_mlda[:train_len]\n",
    "hdp_corpus_test = corpus_mlda[train_len:]\n",
    "\n",
    "hdp_model = HdpModel(hdp_corpus_train, id2word=dictionary_mlda, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic {0}:\n",
      "would like people also could two years first even including considerably last time state back many anxiety southeast seen voters\n",
      "\n",
      "Topic {1}:\n",
      "would last year like bride since also lawyers people years two time company merely collecting new could percent trouble comic\n",
      "\n",
      "Topic {2}:\n",
      "would like people shaping new mother symptoms time also two shots magna instrument first rifle many last worldwide single-family even\n",
      "\n",
      "Topic {3}:\n",
      "would fun like tables injured last weekend emotionally first version prosecutors also people workout could bullying outset rolled years million\n",
      "\n",
      "Topic {4}:\n",
      "airports like consequence people openness prohibiting rubber wheels also group sex numerous figure assess rising removal mortgages years less allied\n",
      "\n",
      "Topic {5}:\n",
      "would facing last many years people playwright prevent services practice culture space also could underwent much client like post urban\n",
      "\n",
      "Topic {6}:\n",
      "boards would reserved improvement witty picnic charts like people change cookingcare way also commentator two son closet portrait venue movement\n",
      "\n",
      "Topic {7}:\n",
      "neighbors would sign happened called like grabbed display release nights last first elephants also telecommunications cash irregularities misstated flags coordinating\n",
      "\n",
      "Topic {8}:\n",
      "crossed two would lion also constructed long last breed commissioner must guiding people approximately discovered first goals agitated packed announcement\n",
      "\n",
      "Topic {9}:\n",
      "would like people scorer relievers gauge also robbery could portfolio complete existing traditional colors striking accustomed three-bedroom cry officiated undertaking\n",
      "\n",
      "Topic {10}:\n",
      "mechanism acceptance acquire practice would presumed dug steer lawful fans bitter exploration correct longtime withdrawn like awareness antidoping reserves people\n",
      "\n",
      "Topic {11}:\n",
      "specifically informed tough aware striving worth also like underneath mother still would 3-year-old cracks divide competing contributors consequences winner jointly\n",
      "\n",
      "Topic {12}:\n",
      "also visits people sex rich singers tax trim reliable generates account pretending years adorned class accusation facing funding newest accept\n",
      "\n",
      "Topic {13}:\n",
      "two like people would basket aircraft out. drafting shoes separately see. legitimacy tale haunting semester first safely blame much violence\n",
      "\n",
      "Topic {14}:\n",
      "patience ballots lending visitors attending director musicians throwing cure preventing belonging resigned connecting affect angrily regards law proposition spur raises\n",
      "\n",
      "Topic {15}:\n",
      "chopped atomic arc mobile like lows perspective to. nearly fixing columnist rented efforts slipped reflection impressed taste viral starred risk\n",
      "\n",
      "Topic {16}:\n",
      "sung official indications tourists company dominated underscores birds include box ink tainted southwestern professors three-story helmet recognize fled market circles\n",
      "\n",
      "Topic {17}:\n",
      "tickets duties foreigners burden progress cloud sounds says imagery size like magazines document assured also salt sweeping machinery whipped hardware\n",
      "\n",
      "Topic {18}:\n",
      "header collaborator again. protesting mirrored fodder print damage founded publishers accustomed uphill russian far-right editors fabrics years subsequently encourage testified\n",
      "\n",
      "Topic {19}:\n",
      "would like maneuver instituted petition somebody also intensified bulk nearby stance paralyzed echoed 19th homered census years made readers briefing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the topic-word distribution\n",
    "get_topics(hdp_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The value of total likelihood obtained by evaluating the model for all documents in the test corpus\n",
    "likelihood = hdp_model.evaluate_test_corpus(hdp_corpus_test)\n",
    "# log_perplexity = (-1 * (likelihood/len(vocab)))\n",
    "log_perplexity = (-1 * (likelihood/len(dictionary_mlda)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for loglikelihood: -4803594.303194, log_perplexity: 490.062671 \n"
     ]
    }
   ],
   "source": [
    "print (\"Values for loglikelihood: %f, log_perplexity: %f \" % (likelihood, log_perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference :\n",
    "- On inspection the topics generated by Hierarchial Dirichlet Process model appear to be overlaping\n",
    "- **Topic-word distributions** are not as orthogonal and meaningful when compared to OLDA models v1 and v2 above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshringi/anaconda3/envs/py3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.7 s, sys: 1.34 s, total: 59.1 s\n",
      "Wall time: 22min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "from pyLDAvis import gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "# vis = gensim.prepare(lda_v1, corpus, dictionary)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
